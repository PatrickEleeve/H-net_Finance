#!/usr/bin/env python3
"""
é’ˆå¯¹å°æ•°æ®é›†çš„ä¼˜åŒ–è®­ç»ƒé…ç½®
"""

import torch
import torch.nn as nn
from hnet_stock_training import HNetConfig, StockTrainer, StockDataset
import json
import os

class SmallDatasetConfig:
    """é’ˆå¯¹å°æ•°æ®é›†çš„ä¼˜åŒ–é…ç½®"""
    
    @staticmethod
    def create_optimized_configs():
        """åˆ›å»ºé’ˆå¯¹å°æ•°æ®é›†ä¼˜åŒ–çš„è®­ç»ƒé…ç½®"""
        
        configs = {
            'nano': {
                'name': 'Nano - æå°æ¨¡å‹ (é˜²è¿‡æ‹Ÿåˆ)',
                'd_model': 64,
                'num_stages': 1,
                'encoder_layers': 2,
                'decoder_layers': 2,
                'main_layers': 4,
                'chunk_ratios': [2],
                'batch_size': 16,
                'learning_rate': 1e-3,
                'max_epochs': 8,
                'dropout': 0.3,
                'weight_decay': 1e-3,
                'params_estimate': '0.3M',
                'description': 'æœ€å°æ¨¡å‹ï¼Œé€‚åˆ647ä¸ªæ ·æœ¬'
            },
            
            'micro': {
                'name': 'Micro - å¾®å‹æ¨¡å‹',
                'd_model': 96,
                'num_stages': 1,
                'encoder_layers': 2,
                'decoder_layers': 2,
                'main_layers': 6,
                'chunk_ratios': [3],
                'batch_size': 12,
                'learning_rate': 8e-4,
                'max_epochs': 12,
                'dropout': 0.25,
                'weight_decay': 5e-4,
                'params_estimate': '0.6M',
                'description': 'å¾®å‹æ¨¡å‹ï¼Œä¿å®ˆè®­ç»ƒ'
            },
            
            'small_regularized': {
                'name': 'Small Regularized - å°æ¨¡å‹+å¼ºæ­£åˆ™åŒ–',
                'd_model': 128,
                'num_stages': 1,
                'encoder_layers': 3,
                'decoder_layers': 3,
                'main_layers': 6,
                'chunk_ratios': [4],
                'batch_size': 8,
                'learning_rate': 5e-4,
                'max_epochs': 15,
                'dropout': 0.4,
                'weight_decay': 1e-3,
                'params_estimate': '0.9M',
                'description': 'å°æ¨¡å‹é…åˆå¼ºæ­£åˆ™åŒ–'
            },
            
            'early_stop': {
                'name': 'Early Stop - æ—©åœç­–ç•¥',
                'd_model': 128,
                'num_stages': 2,
                'encoder_layers': 3,
                'decoder_layers': 3,
                'main_layers': 8,
                'chunk_ratios': [4, 3],
                'batch_size': 8,
                'learning_rate': 3e-4,
                'max_epochs': 50,  # é«˜epochä½†ä¼šæ—©åœ
                'dropout': 0.35,
                'weight_decay': 8e-4,
                'early_stopping': True,
                'patience': 5,
                'params_estimate': '1.2M',
                'description': 'ä½¿ç”¨æ—©åœé¿å…è¿‡æ‹Ÿåˆ'
            }
        }
        
        return configs

class EarlyStoppingCallback:
    """æ—©åœå›è°ƒ"""
    
    def __init__(self, patience=5, min_delta=0.001, restore_best=True):
        self.patience = patience
        self.min_delta = min_delta
        self.restore_best = restore_best
        self.best_loss = float('inf')
        self.counter = 0
        self.best_weights = None
        
    def __call__(self, val_loss, model):
        if val_loss < self.best_loss - self.min_delta:
            self.best_loss = val_loss
            self.counter = 0
            if self.restore_best:
                self.best_weights = model.state_dict().copy()
        else:
            self.counter += 1
            
        return self.counter >= self.patience
    
    def restore_best_weights(self, model):
        if self.best_weights is not None:
            model.load_state_dict(self.best_weights)

class RegularizedStockTrainer(StockTrainer):
    """å¢å¼ºæ­£åˆ™åŒ–çš„è®­ç»ƒå™¨"""
    
    def __init__(self, config, device='cpu', use_early_stopping=False):
        super().__init__(config, device)
        
        self.use_early_stopping = use_early_stopping
        if use_early_stopping:
            self.early_stopping = EarlyStoppingCallback(
                patience=getattr(config, 'patience', 5),
                min_delta=0.001
            )
        
        # å¢å¼ºæ­£åˆ™åŒ–
        self.setup_regularization(config)
    
    def setup_regularization(self, config):
        """è®¾ç½®æ­£åˆ™åŒ–"""
        # æ›´å¼ºçš„æƒé‡è¡°å‡
        for param_group in self.optimizer.param_groups:
            param_group['weight_decay'] = config.weight_decay
        
        # Dropoutè°ƒåº¦å™¨
        self.dropout_scheduler = self.create_dropout_scheduler(config.dropout)
    
    def create_dropout_scheduler(self, max_dropout):
        """åˆ›å»ºåŠ¨æ€dropoutè°ƒåº¦å™¨"""
        def scheduler(epoch):
            # éšç€è®­ç»ƒè¿›è¡Œé€æ¸å¢åŠ dropout
            return min(max_dropout, 0.1 + (max_dropout - 0.1) * epoch / 20)
        return scheduler
    
    def apply_dropout_schedule(self, epoch):
        """åº”ç”¨dropoutè°ƒåº¦"""
        current_dropout = self.dropout_scheduler(epoch)
        
        # æ›´æ–°æ¨¡å‹ä¸­çš„dropout
        for module in self.model.modules():
            if isinstance(module, nn.Dropout):
                module.p = current_dropout
    
    def train_epoch(self, train_loader, epoch):
        """è®­ç»ƒä¸€ä¸ªepochï¼Œå¢å¼ºæ­£åˆ™åŒ–"""
        self.model.train()
        
        # åº”ç”¨dropoutè°ƒåº¦
        self.apply_dropout_schedule(epoch)
        
        total_loss = 0
        num_batches = 0
        
        for batch_idx, batch in enumerate(train_loader):
            # æ•°æ®åˆ°è®¾å¤‡
            price_data = batch['price'].to(self.device)
            technical_data = batch['technical'].to(self.device)
            news_data = batch['news'].to(self.device)
            targets = {k: v.to(self.device) for k, v in batch['targets'].items()}
            
            # å‰å‘ä¼ æ’­
            self.optimizer.zero_grad()
            outputs = self.model(price_data, technical_data, news_data)
            
            # è®¡ç®—æŸå¤±
            losses = self.compute_losses(outputs, targets)
            total_loss_batch = sum(losses.values())
            
            # æ·»åŠ L2æ­£åˆ™åŒ–
            l2_reg = self.compute_l2_regularization()
            total_loss_batch += l2_reg
            
            # åå‘ä¼ æ’­
            total_loss_batch.backward()
            
            # æ¢¯åº¦è£å‰ªï¼ˆé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼‰
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            
            self.optimizer.step()
            
            total_loss += total_loss_batch.item()
            num_batches += 1
        
        return total_loss / num_batches
    
    def compute_l2_regularization(self):
        """è®¡ç®—L2æ­£åˆ™åŒ–"""
        l2_reg = 0
        for param in self.model.parameters():
            l2_reg += torch.norm(param, 2) ** 2
        return l2_reg * self.config.weight_decay
    
    def train(self, train_dataset, val_dataset):
        """è®­ç»ƒæ¨¡å‹ï¼Œæ”¯æŒæ—©åœ"""
        from torch.utils.data import DataLoader
        
        train_loader = DataLoader(
            train_dataset, 
            batch_size=self.config.batch_size, 
            shuffle=True,
            num_workers=0
        )
        val_loader = DataLoader(
            val_dataset, 
            batch_size=self.config.batch_size, 
            shuffle=False,
            num_workers=0
        )
        
        print(f"å¼€å§‹è®­ç»ƒï¼Œæœ€å¤§epoch: {self.config.max_epochs}")
        if self.use_early_stopping:
            print(f"ä½¿ç”¨æ—©åœæœºåˆ¶ï¼Œè€å¿ƒåº¦: {self.early_stopping.patience}")
        
        for epoch in range(self.config.max_epochs):
            # è®­ç»ƒ
            train_loss = self.train_epoch(train_loader, epoch)
            
            # éªŒè¯
            val_losses = self.validate(val_loader)
            val_loss = val_losses['total']
            
            print(f"Epoch {epoch+1:3d}: Train Loss: {train_loss:.4f}, "
                  f"Val Loss: {val_loss:.4f}")
            
            # å­¦ä¹ ç‡è°ƒåº¦
            self.scheduler.step(val_loss)
            
            # æ—©åœæ£€æŸ¥
            if self.use_early_stopping:
                if self.early_stopping(val_loss, self.model):
                    print(f"Early stopping at epoch {epoch+1}")
                    self.early_stopping.restore_best_weights(self.model)
                    break
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_loss < getattr(self, 'best_val_loss', float('inf')):
                self.best_val_loss = val_loss
                torch.save({
                    'model_state_dict': self.model.state_dict(),
                    'optimizer_state_dict': self.optimizer.state_dict(),
                    'scheduler_state_dict': self.scheduler.state_dict(),
                    'val_loss': val_loss,
                    'config': self.config,
                    'epoch': epoch
                }, 'best_small_dataset_model.pth')

def create_small_data_trainer(mode='nano', data_dir='stock_data_eodhd_extended'):
    """åˆ›å»ºé’ˆå¯¹å°æ•°æ®é›†çš„è®­ç»ƒå™¨"""
    
    configs = SmallDatasetConfig.create_optimized_configs()
    
    if mode not in configs:
        print(f"æœªçŸ¥æ¨¡å¼: {mode}")
        print(f"å¯ç”¨æ¨¡å¼: {list(configs.keys())}")
        return None
    
    config_dict = configs[mode]
    
    # åˆ›å»ºHNetConfig
    config = HNetConfig(
        d_model=config_dict['d_model'],
        num_stages=config_dict['num_stages'],
        encoder_layers=config_dict['encoder_layers'],
        decoder_layers=config_dict['decoder_layers'],
        main_layers=config_dict['main_layers'],
        chunk_ratios=config_dict['chunk_ratios'],
        batch_size=config_dict['batch_size'],
        learning_rate=config_dict['learning_rate'],
        max_epochs=config_dict['max_epochs'],
        dropout=config_dict['dropout'],
        weight_decay=config_dict['weight_decay'],
        sequence_length=60,
        prediction_horizon=5
    )
    
    # æ·»åŠ é¢å¤–é…ç½®
    if 'early_stopping' in config_dict:
        config.early_stopping = config_dict['early_stopping']
        config.patience = config_dict.get('patience', 5)
    
    # åˆ›å»ºè®­ç»ƒå™¨
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    use_early_stopping = getattr(config, 'early_stopping', False)
    
    trainer = RegularizedStockTrainer(config, device, use_early_stopping)
    
    print(f"âœ… åˆ›å»º {config_dict['name']} è®­ç»ƒå™¨")
    print(f"   æ¨¡å‹å‚æ•°: {config_dict['params_estimate']}")
    print(f"   æè¿°: {config_dict['description']}")
    
    return trainer, config

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ å°æ•°æ®é›†ä¼˜åŒ–è®­ç»ƒè§£å†³æ–¹æ¡ˆ")
    print("=" * 50)
    
    # æ˜¾ç¤ºé…ç½®é€‰é¡¹
    configs = SmallDatasetConfig.create_optimized_configs()
    
    print("ğŸ“‹ å¯ç”¨çš„å°æ•°æ®é›†ä¼˜åŒ–é…ç½®:")
    for i, (key, config) in enumerate(configs.items(), 1):
        print(f"  {i}. {key}: {config['name']}")
        print(f"     - {config['description']}")
        print(f"     - å‚æ•°é‡: {config['params_estimate']}")
        print(f"     - Epoch: {config['max_epochs']}")
        print()
    
    # ç”¨æˆ·é€‰æ‹©
    try:
        choice = input("è¯·é€‰æ‹©é…ç½® (1-4): ").strip()
        mode_keys = list(configs.keys())
        mode = mode_keys[int(choice) - 1]
    except (ValueError, IndexError):
        print("æ— æ•ˆé€‰æ‹©ï¼Œä½¿ç”¨é»˜è®¤nanoæ¨¡å¼")
        mode = 'nano'
    
    print(f"\nğŸš€ ä½¿ç”¨ {mode} é…ç½®è¿›è¡Œè®­ç»ƒ")
    
    # æ£€æŸ¥æ•°æ®
    data_options = []
    if os.path.exists("stock_data_eodhd_extended"):
        data_options.append(("stock_data_eodhd_extended", "EODHDæ‰©å±•æ•°æ®"))
    if os.path.exists("augmented_data"):
        data_options.append(("augmented_data", "å¢å¼ºæ•°æ®"))
    
    if not data_options:
        print("âŒ æœªæ‰¾åˆ°è®­ç»ƒæ•°æ®")
        print("è¯·å…ˆè¿è¡Œæ•°æ®é¢„å¤„ç†æˆ–æ•°æ®å¢å¼º")
        return
    
    print(f"\nğŸ“‚ å¯ç”¨æ•°æ®:")
    for i, (path, desc) in enumerate(data_options, 1):
        print(f"  {i}. {desc} ({path})")
    
    try:
        data_choice = input("è¯·é€‰æ‹©æ•°æ® (1-2): ").strip()
        data_dir = data_options[int(data_choice) - 1][0]
    except (ValueError, IndexError):
        data_dir = data_options[0][0]
    
    print(f"ä½¿ç”¨æ•°æ®: {data_dir}")
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer, config = create_small_data_trainer(mode, data_dir)
    
    if trainer is None:
        return
    
    try:
        # åˆ›å»ºæ•°æ®é›†
        train_dataset = StockDataset(data_dir, config, 'train')
        val_dataset = StockDataset(data_dir, config, 'val')
        
        print(f"\nğŸ“Š æ•°æ®é›†ä¿¡æ¯:")
        print(f"   è®­ç»ƒæ ·æœ¬: {len(train_dataset)}")
        print(f"   éªŒè¯æ ·æœ¬: {len(val_dataset)}")
        
        # è®¡ç®—è¿‡æ‹Ÿåˆé£é™©
        params_count = sum(p.numel() for p in trainer.model.parameters())
        samples_per_param = len(train_dataset) / params_count
        
        print(f"   æ¨¡å‹å‚æ•°: {params_count:,}")
        print(f"   æ¯å‚æ•°æ ·æœ¬æ•°: {samples_per_param:.6f}")
        
        if samples_per_param < 0.001:
            print("   âš ï¸  è¿‡æ‹Ÿåˆé£é™©: ğŸ”´ é«˜")
        else:
            print("   âœ… è¿‡æ‹Ÿåˆé£é™©: ğŸŸ¢ å¯æ§")
        
        # å¼€å§‹è®­ç»ƒ
        response = input(f"\nå¼€å§‹è®­ç»ƒ? (Y/n): ").strip().lower()
        if response != 'n':
            trainer.train(train_dataset, val_dataset)
            print(f"\nâœ… è®­ç»ƒå®Œæˆ!")
            print(f"æ¨¡å‹å·²ä¿å­˜ä¸º: best_small_dataset_model.pth")
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
