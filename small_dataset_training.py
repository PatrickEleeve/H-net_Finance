#!/usr/bin/env python3
"""
é’ˆå¯¹å°æ•°æ®é›†çš„ä¼˜åŒ–è®­ç»ƒé…ç½® - å¢å¼ºç‰ˆæœ¬ï¼ŒåŒ…å«è¯¦ç»†è®­ç»ƒç›‘æ§
"""

import torch
import torch.nn as nn
from hnet_stock_training import HNetConfig, StockTrainer, StockDataset
import json
import os
import time
import sys
from datetime import datetime, timedelta
from collections import defaultdict, deque

class SmallDatasetConfig:
    """é’ˆå¯¹å°æ•°æ®é›†çš„ä¼˜åŒ–é…ç½®"""
    
    @staticmethod
    def create_optimized_configs():
        """åˆ›å»ºé’ˆå¯¹å°æ•°æ®é›†ä¼˜åŒ–çš„è®­ç»ƒé…ç½®"""
        
        configs = {
            'nano': {
                'name': 'Nano - æå°æ¨¡å‹ (é˜²è¿‡æ‹Ÿåˆ)',
                'd_model': 64,
                'num_stages': 1,
                'encoder_layers': 2,
                'decoder_layers': 2,
                'main_layers': 4,
                'chunk_ratios': [2],
                'batch_size': 16,
                'learning_rate': 1e-3,
                'max_epochs': 8,
                'dropout': 0.3,
                'weight_decay': 1e-3,
                'params_estimate': '0.3M',
                'description': 'æœ€å°æ¨¡å‹ï¼Œé€‚åˆ647ä¸ªæ ·æœ¬'
            },
            
            'micro': {
                'name': 'Micro - å¾®å‹æ¨¡å‹',
                'd_model': 96,
                'num_stages': 1,
                'encoder_layers': 2,
                'decoder_layers': 2,
                'main_layers': 6,
                'chunk_ratios': [3],
                'batch_size': 12,
                'learning_rate': 8e-4,
                'max_epochs': 12,
                'dropout': 0.25,
                'weight_decay': 5e-4,
                'params_estimate': '0.6M',
                'description': 'å¾®å‹æ¨¡å‹ï¼Œä¿å®ˆè®­ç»ƒ'
            },
            
            'small_regularized': {
                'name': 'Small Regularized - å°æ¨¡å‹+å¼ºæ­£åˆ™åŒ–',
                'd_model': 128,
                'num_stages': 1,
                'encoder_layers': 3,
                'decoder_layers': 3,
                'main_layers': 6,
                'chunk_ratios': [4],
                'batch_size': 8,
                'learning_rate': 5e-4,
                'max_epochs': 15,
                'dropout': 0.4,
                'weight_decay': 1e-3,
                'params_estimate': '0.9M',
                'description': 'å°æ¨¡å‹é…åˆå¼ºæ­£åˆ™åŒ–'
            },
            
            'early_stop': {
                'name': 'Early Stop - æ—©åœç­–ç•¥',
                'd_model': 128,
                'num_stages': 2,
                'encoder_layers': 3,
                'decoder_layers': 3,
                'main_layers': 8,
                'chunk_ratios': [4, 3],
                'batch_size': 8,
                'learning_rate': 3e-4,
                'max_epochs': 50,  # é«˜epochä½†ä¼šæ—©åœ
                'dropout': 0.35,
                'weight_decay': 8e-4,
                'early_stopping': True,
                'patience': 5,
                'params_estimate': '1.2M',
                'description': 'ä½¿ç”¨æ—©åœé¿å…è¿‡æ‹Ÿåˆ'
            }
        }
        
        return configs

class TrainingMonitor:
    """è®­ç»ƒç›‘æ§å™¨ - æä¾›è¯¦ç»†çš„è®­ç»ƒè¿‡ç¨‹å¯è§†åŒ–"""
    
    def __init__(self):
        self.epoch_start_time = None
        self.batch_start_time = None
        self.loss_history = defaultdict(list)
        self.speed_history = deque(maxlen=50)  # ä¿ç•™æœ€è¿‘50ä¸ªæ‰¹æ¬¡çš„é€Ÿåº¦
        self.total_batches = 0
        self.current_epoch = 0
        
    def start_epoch(self, epoch, total_batches):
        """å¼€å§‹æ–°çš„epoch"""
        self.current_epoch = epoch
        self.total_batches = total_batches
        self.epoch_start_time = time.time()
        print(f"\n{'='*60}")
        print(f"ğŸš€ Epoch {epoch+1} å¼€å§‹ ({datetime.now().strftime('%H:%M:%S')})")
        print(f"{'='*60}")
        
    def start_batch(self, batch_idx):
        """å¼€å§‹æ–°çš„æ‰¹æ¬¡"""
        self.batch_start_time = time.time()
        
    def end_batch(self, batch_idx, losses, batch_size):
        """ç»“æŸæ‰¹æ¬¡ï¼Œè®°å½•ä¿¡æ¯"""
        if self.batch_start_time is None:
            return
            
        batch_time = time.time() - self.batch_start_time
        samples_per_sec = batch_size / batch_time if batch_time > 0 else 0
        self.speed_history.append(samples_per_sec)
        
        # è®°å½•æŸå¤±
        for key, value in losses.items():
            if isinstance(value, torch.Tensor):
                self.loss_history[key].append(value.item())
            else:
                self.loss_history[key].append(float(value))
        
        # è®¡ç®—è¿›åº¦
        progress = (batch_idx + 1) / self.total_batches
        
        # è®¡ç®—é¢„ä¼°å‰©ä½™æ—¶é—´
        if len(self.speed_history) > 0:
            avg_speed = sum(self.speed_history) / len(self.speed_history)
            remaining_batches = self.total_batches - (batch_idx + 1)
            remaining_time = remaining_batches * batch_time if avg_speed > 0 else 0
            eta = datetime.now() + timedelta(seconds=remaining_time)
        else:
            eta = None
            avg_speed = 0
        
        # æ‰“å°è¯¦ç»†ä¿¡æ¯
        if (batch_idx + 1) % max(1, self.total_batches // 10) == 0 or batch_idx == 0:
            self._print_batch_info(batch_idx, progress, losses, samples_per_sec, eta)
    
    def _print_batch_info(self, batch_idx, progress, losses, speed, eta):
        """æ‰“å°æ‰¹æ¬¡ä¿¡æ¯"""
        # è¿›åº¦æ¡
        bar_length = 40
        filled_length = int(bar_length * progress)
        bar = 'â–ˆ' * filled_length + 'â–“' * (bar_length - filled_length)
        
        # ä¸»è¦æŸå¤±
        total_loss = losses.get('total', 0)
        if isinstance(total_loss, torch.Tensor):
            total_loss = total_loss.item()
        
        print(f"\ræ‰¹æ¬¡ {batch_idx+1:4d}/{self.total_batches} |{bar}| "
              f"{progress*100:5.1f}% | "
              f"æŸå¤±: {total_loss:.4f} | "
              f"é€Ÿåº¦: {speed:.1f} samples/s", end="")
        
        # æ¯20ä¸ªæ‰¹æ¬¡è¯¦ç»†è¾“å‡º
        if (batch_idx + 1) % max(1, self.total_batches // 5) == 0:
            print()  # æ¢è¡Œ
            self._print_detailed_losses(losses)
            if eta:
                print(f"   â° é¢„è®¡å®Œæˆ: {eta.strftime('%H:%M:%S')}")
                print(f"   ğŸƒ å½“å‰é€Ÿåº¦: {speed:.1f} samples/s")
    
    def _print_detailed_losses(self, losses):
        """æ‰“å°è¯¦ç»†æŸå¤±åˆ†è§£"""
        print("   ğŸ“Š æŸå¤±åˆ†è§£:", end="")
        for key, value in losses.items():
            if key != 'total':
                if isinstance(value, torch.Tensor):
                    value = value.item()
                print(f" {key}={value:.4f}", end="")
        print()
    
    def end_epoch(self, train_loss, val_losses):
        """ç»“æŸepoch"""
        if self.epoch_start_time is None:
            return
            
        epoch_time = time.time() - self.epoch_start_time
        
        print(f"\n{'='*60}")
        print(f"âœ… Epoch {self.current_epoch+1} å®Œæˆ")
        print(f"   â±ï¸  ç”¨æ—¶: {epoch_time:.2f}ç§’")
        print(f"   ğŸ“ˆ è®­ç»ƒæŸå¤±: {train_loss:.6f}")
        
        # éªŒè¯æŸå¤±è¯¦æƒ…
        if isinstance(val_losses, dict):
            print(f"   ğŸ“‰ éªŒè¯æŸå¤±: {val_losses.get('total', 0):.6f}")
            for key, value in val_losses.items():
                if key != 'total':
                    if isinstance(value, torch.Tensor):
                        value = value.item()
                    print(f"      - {key}: {value:.6f}")
        else:
            print(f"   ğŸ“‰ éªŒè¯æŸå¤±: {val_losses:.6f}")
        
        # GPUå†…å­˜ä½¿ç”¨
        if torch.cuda.is_available():
            memory_used = torch.cuda.memory_allocated() / 1024**2
            memory_cached = torch.cuda.memory_reserved() / 1024**2
            print(f"   ğŸ’¾ GPUå†…å­˜: {memory_used:.1f}MB å·²ç”¨, {memory_cached:.1f}MB ç¼“å­˜")
        
        print(f"{'='*60}")

class EarlyStoppingCallback:
    """æ—©åœå›è°ƒ"""
    
    def __init__(self, patience=5, min_delta=0.001, restore_best=True):
        self.patience = patience
        self.min_delta = min_delta
        self.restore_best = restore_best
        self.best_loss = float('inf')
        self.counter = 0
        self.best_weights = None
        
    def __call__(self, val_loss, model):
        if val_loss < self.best_loss - self.min_delta:
            self.best_loss = val_loss
            self.counter = 0
            if self.restore_best:
                self.best_weights = model.state_dict().copy()
        else:
            self.counter += 1
            
        return self.counter >= self.patience
    
    def restore_best_weights(self, model):
        if self.best_weights is not None:
            model.load_state_dict(self.best_weights)

class RegularizedStockTrainer(StockTrainer):
    """å¢å¼ºæ­£åˆ™åŒ–çš„è®­ç»ƒå™¨"""
    
    def __init__(self, config, device='cpu', use_early_stopping=False):
        super().__init__(config, device)
        
        self.use_early_stopping = use_early_stopping
        if use_early_stopping:
            self.early_stopping = EarlyStoppingCallback(
                patience=getattr(config, 'patience', 5),
                min_delta=0.001
            )
        
        # å¢å¼ºæ­£åˆ™åŒ–
        self.setup_regularization(config)
    
    def setup_regularization(self, config):
        """è®¾ç½®æ­£åˆ™åŒ–"""
        # æ›´å¼ºçš„æƒé‡è¡°å‡
        for param_group in self.optimizer.param_groups:
            param_group['weight_decay'] = config.weight_decay
        
        # Dropoutè°ƒåº¦å™¨
        self.dropout_scheduler = self.create_dropout_scheduler(config.dropout)
    
    def create_dropout_scheduler(self, max_dropout):
        """åˆ›å»ºåŠ¨æ€dropoutè°ƒåº¦å™¨"""
        def scheduler(epoch):
            # éšç€è®­ç»ƒè¿›è¡Œé€æ¸å¢åŠ dropout
            return min(max_dropout, 0.1 + (max_dropout - 0.1) * epoch / 20)
        return scheduler
    
    def apply_dropout_schedule(self, epoch):
        """åº”ç”¨dropoutè°ƒåº¦"""
        current_dropout = self.dropout_scheduler(epoch)
        
        # æ›´æ–°æ¨¡å‹ä¸­çš„dropout
        for module in self.model.modules():
            if isinstance(module, nn.Dropout):
                module.p = current_dropout
    
    def compute_l2_regularization(self):
        """è®¡ç®—L2æ­£åˆ™åŒ–"""
        l2_reg = torch.tensor(0.0, device=self.device)
        for param in self.model.parameters():
            l2_reg += torch.norm(param, 2) ** 2
        return l2_reg * self.config.weight_decay
    
    def train_epoch(self, dataloader):
        """è®­ç»ƒä¸€ä¸ªepochï¼Œå¢å¼ºæ­£åˆ™åŒ–å’Œè¯¦ç»†ç›‘æ§"""
        self.model.train()
        
        # åˆå§‹åŒ–ç›‘æ§
        if not hasattr(self, 'monitor'):
            self.monitor = TrainingMonitor()
        
        total_losses = {'total': 0, 'price': 0, 'volatility': 0, 'direction': 0, 'boundary': 0, 'l2_reg': 0}
        num_batches = len(dataloader)
        
        # å¼€å§‹epochç›‘æ§
        current_epoch = getattr(self, 'current_training_epoch', 0)
        self.monitor.start_epoch(current_epoch, num_batches)
        
        for batch_idx, batch in enumerate(dataloader):
            # å¼€å§‹æ‰¹æ¬¡ç›‘æ§
            self.monitor.start_batch(batch_idx)
            
            # æ•°æ®åˆ°è®¾å¤‡
            price_data = batch['price'].to(self.device)
            technical_data = batch['technical'].to(self.device)
            news_data = batch['news'].to(self.device)
            targets = {k: v.to(self.device) for k, v in batch['targets'].items()}
            
            # å‰å‘ä¼ æ’­
            self.optimizer.zero_grad()
            predictions, boundary_loss = self.model(price_data, technical_data, news_data)
            
            # è®¡ç®—æŸå¤±
            losses = self.compute_loss(predictions, targets, boundary_loss)
            
            # æ·»åŠ L2æ­£åˆ™åŒ–
            l2_reg = self.compute_l2_regularization()
            losses['total'] = losses['total'] + l2_reg
            losses['l2_reg'] = l2_reg
            
            # åå‘ä¼ æ’­
            losses['total'].backward()
            
            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            
            self.optimizer.step()
            
            # è®°å½•æŸå¤±
            batch_losses = {k: v.item() for k, v in losses.items()}
            
            for k, v in batch_losses.items():
                if k in total_losses:
                    total_losses[k] += v
            
            # ç»“æŸæ‰¹æ¬¡ç›‘æ§
            self.monitor.end_batch(batch_idx, batch_losses, len(price_data))
        
        # è®¡ç®—å¹³å‡æŸå¤±
        avg_losses = {}
        for key in total_losses:
            avg_losses[key] = total_losses[key] / num_batches
        
        return avg_losses
    
    def train(self, train_dataset, val_dataset):
        """è®­ç»ƒæ¨¡å‹ï¼Œæ”¯æŒæ—©åœ"""
        from torch.utils.data import DataLoader
        
        train_loader = DataLoader(
            train_dataset, 
            batch_size=self.config.batch_size, 
            shuffle=True,
            num_workers=0
        )
        val_loader = DataLoader(
            val_dataset, 
            batch_size=self.config.batch_size, 
            shuffle=False,
            num_workers=0
        )
        
        print(f"å¼€å§‹è®­ç»ƒï¼Œæœ€å¤§epoch: {self.config.max_epochs}")
        if self.use_early_stopping:
            print(f"ä½¿ç”¨æ—©åœæœºåˆ¶ï¼Œè€å¿ƒåº¦: {self.early_stopping.patience}")
        
        for epoch in range(self.config.max_epochs):
            # è®¾ç½®å½“å‰epochç”¨äºç›‘æ§
            self.current_training_epoch = epoch
            
            # è®­ç»ƒ
            train_losses = self.train_epoch(train_loader)
            train_loss = train_losses['total']
            
            # éªŒè¯
            val_losses = self.validate(val_loader)
            val_loss = val_losses['total']
            
            # ç»“æŸepochç›‘æ§
            if hasattr(self, 'monitor'):
                self.monitor.end_epoch(train_loss, val_losses)
            
            # å­¦ä¹ ç‡è°ƒåº¦
            self.scheduler.step(val_loss)
            
            # æ—©åœæ£€æŸ¥
            if self.use_early_stopping:
                if self.early_stopping(val_loss, self.model):
                    print(f"Early stopping at epoch {epoch+1}")
                    self.early_stopping.restore_best_weights(self.model)
                    break
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_loss < getattr(self, 'best_val_loss', float('inf')):
                self.best_val_loss = val_loss
                torch.save({
                    'model_state_dict': self.model.state_dict(),
                    'optimizer_state_dict': self.optimizer.state_dict(),
                    'scheduler_state_dict': self.scheduler.state_dict(),
                    'val_loss': val_loss,
                    'config': self.config,
                    'epoch': epoch
                }, 'best_small_dataset_model.pth')

class ExtendedHNetConfig(HNetConfig):
    """æ‰©å±•çš„é…ç½®ç±»ï¼Œæ”¯æŒæ—©åœç­‰åŠŸèƒ½"""
    def __init__(self, **kwargs):
        super().__init__()
        # è®¾ç½®é»˜è®¤çš„æ—©åœç›¸å…³å±æ€§
        self.early_stopping = False
        self.patience = 5
        
        # åº”ç”¨ä¼ å…¥çš„é…ç½®
        for key, value in kwargs.items():
            setattr(self, key, value)

def create_small_data_trainer(mode='nano', data_dir='stock_data_eodhd_extended'):
    """åˆ›å»ºé’ˆå¯¹å°æ•°æ®é›†çš„è®­ç»ƒå™¨"""
    
    configs = SmallDatasetConfig.create_optimized_configs()
    
    if mode not in configs:
        print(f"æœªçŸ¥æ¨¡å¼: {mode}")
        print(f"å¯ç”¨æ¨¡å¼: {list(configs.keys())}")
        return None, None
    
    config_dict = configs[mode]
    
    # åˆ›å»ºæ‰©å±•çš„HNetConfig
    config = ExtendedHNetConfig(
        d_model=config_dict['d_model'],
        num_stages=config_dict['num_stages'],
        encoder_layers=config_dict['encoder_layers'],
        decoder_layers=config_dict['decoder_layers'],
        main_layers=config_dict['main_layers'],
        chunk_ratios=config_dict['chunk_ratios'],
        batch_size=config_dict['batch_size'],
        learning_rate=config_dict['learning_rate'],
        max_epochs=config_dict['max_epochs'],
        dropout=config_dict['dropout'],
        weight_decay=config_dict['weight_decay'],
        sequence_length=60,
        prediction_horizon=5
    )
    
    # æ·»åŠ é¢å¤–é…ç½®
    if 'early_stopping' in config_dict:
        config.early_stopping = config_dict['early_stopping']
        config.patience = config_dict.get('patience', 5)
    
    # åˆ›å»ºè®­ç»ƒå™¨
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    use_early_stopping = getattr(config, 'early_stopping', False)
    
    trainer = RegularizedStockTrainer(config, device, use_early_stopping)
    
    print(f"âœ… åˆ›å»º {config_dict['name']} è®­ç»ƒå™¨")
    print(f"   æ¨¡å‹å‚æ•°: {config_dict['params_estimate']}")
    print(f"   æè¿°: {config_dict['description']}")
    
    return trainer, config

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ å°æ•°æ®é›†ä¼˜åŒ–è®­ç»ƒè§£å†³æ–¹æ¡ˆ")
    print("=" * 50)
    
    # æ˜¾ç¤ºé…ç½®é€‰é¡¹
    configs = SmallDatasetConfig.create_optimized_configs()
    
    print("ğŸ“‹ å¯ç”¨çš„å°æ•°æ®é›†ä¼˜åŒ–é…ç½®:")
    for i, (key, config) in enumerate(configs.items(), 1):
        print(f"  {i}. {key}: {config['name']}")
        print(f"     - {config['description']}")
        print(f"     - å‚æ•°é‡: {config['params_estimate']}")
        print(f"     - Epoch: {config['max_epochs']}")
        print()
    
    # ç”¨æˆ·é€‰æ‹©
    try:
        choice = input("è¯·é€‰æ‹©é…ç½® (1-4): ").strip()
        mode_keys = list(configs.keys())
        mode = mode_keys[int(choice) - 1]
    except (ValueError, IndexError):
        print("æ— æ•ˆé€‰æ‹©ï¼Œä½¿ç”¨é»˜è®¤nanoæ¨¡å¼")
        mode = 'nano'
    
    print(f"\nğŸš€ ä½¿ç”¨ {mode} é…ç½®è¿›è¡Œè®­ç»ƒ")
    
    # æ£€æŸ¥æ•°æ®
    data_options = []
    if os.path.exists("stock_data_eodhd_extended"):
        data_options.append(("stock_data_eodhd_extended", "EODHDæ‰©å±•æ•°æ®"))
    if os.path.exists("augmented_data"):
        data_options.append(("augmented_data", "å¢å¼ºæ•°æ®"))
    
    if not data_options:
        print("âŒ æœªæ‰¾åˆ°è®­ç»ƒæ•°æ®")
        print("è¯·å…ˆè¿è¡Œæ•°æ®é¢„å¤„ç†æˆ–æ•°æ®å¢å¼º")
        return
    
    print(f"\nğŸ“‚ å¯ç”¨æ•°æ®:")
    for i, (path, desc) in enumerate(data_options, 1):
        print(f"  {i}. {desc} ({path})")
    
    try:
        data_choice = input("è¯·é€‰æ‹©æ•°æ® (1-2): ").strip()
        data_dir = data_options[int(data_choice) - 1][0]
    except (ValueError, IndexError):
        data_dir = data_options[0][0]
    
    print(f"ä½¿ç”¨æ•°æ®: {data_dir}")
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer, config = create_small_data_trainer(mode, data_dir)
    
    if trainer is None or config is None:
        print("âŒ åˆ›å»ºè®­ç»ƒå™¨å¤±è´¥")
        return
    
    try:
        # åˆ›å»ºæ•°æ®é›†
        train_dataset = StockDataset(data_dir, config, 'train')
        val_dataset = StockDataset(data_dir, config, 'val')
        
        print(f"\nğŸ“Š æ•°æ®é›†ä¿¡æ¯:")
        print(f"   è®­ç»ƒæ ·æœ¬: {len(train_dataset)}")
        print(f"   éªŒè¯æ ·æœ¬: {len(val_dataset)}")
        
        # è®¡ç®—è¿‡æ‹Ÿåˆé£é™©
        params_count = sum(p.numel() for p in trainer.model.parameters())
        samples_per_param = len(train_dataset) / params_count
        
        print(f"   æ¨¡å‹å‚æ•°: {params_count:,}")
        print(f"   æ¯å‚æ•°æ ·æœ¬æ•°: {samples_per_param:.6f}")
        
        if samples_per_param < 0.001:
            print("   âš ï¸  è¿‡æ‹Ÿåˆé£é™©: ğŸ”´ é«˜")
        else:
            print("   âœ… è¿‡æ‹Ÿåˆé£é™©: ğŸŸ¢ å¯æ§")
        
        # å¼€å§‹è®­ç»ƒ
        response = input(f"\nå¼€å§‹è®­ç»ƒ? (Y/n): ").strip().lower()
        if response != 'n':
            trainer.train(train_dataset, val_dataset)
            print(f"\nâœ… è®­ç»ƒå®Œæˆ!")
            print(f"æ¨¡å‹å·²ä¿å­˜ä¸º: best_small_dataset_model.pth")
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
