# H-Netè‚¡ç¥¨å¸‚åœºå®æ—¶åˆ†æ - å®Œæ•´ä½¿ç”¨æŒ‡å—

## ğŸ“‹ ç›®å½•
1. [ç¯å¢ƒè¦æ±‚](#ç¯å¢ƒè¦æ±‚)
2. [æ•°æ®è¦æ±‚ä¸å‡†å¤‡](#æ•°æ®è¦æ±‚ä¸å‡†å¤‡)
3. [å®‰è£…æ­¥éª¤](#å®‰è£…æ­¥éª¤)
4. [æ•°æ®é¢„å¤„ç†](#æ•°æ®é¢„å¤„ç†)
5. [æ¨¡å‹è®­ç»ƒ](#æ¨¡å‹è®­ç»ƒ)
6. [å®æ—¶æ¨ç†](#å®æ—¶æ¨ç†)
7. [æ€§èƒ½ä¼˜åŒ–](#æ€§èƒ½ä¼˜åŒ–)
8. [æ•…éšœæ’é™¤](#æ•…éšœæ’é™¤)

---

## ğŸ–¥ï¸ ç¯å¢ƒè¦æ±‚

### ç¡¬ä»¶è¦æ±‚
```
æ¨èé…ç½®:
- GPU: NVIDIA RTX 4090 (24GB VRAM) æˆ–æ›´é«˜
- CPU: Intel i9-12900K æˆ– AMD Ryzen 9 5900X
- RAM: 64GB DDR4
- å­˜å‚¨: 1TB NVMe SSD

æœ€ä½é…ç½®:
- GPU: NVIDIA RTX 3080 (10GB VRAM)
- CPU: Intel i7-10700K æˆ– AMD Ryzen 7 3700X
- RAM: 32GB DDR4
- å­˜å‚¨: 500GB SSD
```

### è½¯ä»¶ç¯å¢ƒ
```
æ“ä½œç³»ç»Ÿ: Ubuntu 20.04+ / Windows 10+ / macOS 12+
Python: 3.9-3.11
CUDA: 11.8+ (for GPU training)
Docker: 20.10+ (å¯é€‰)
```

---

## ğŸ“Š æ•°æ®è¦æ±‚ä¸å‡†å¤‡

### æ•°æ®æ ¼å¼è¦æ±‚

#### 1. ä»·æ ¼æ•°æ® (OHLCVA)
```python
# CSVæ ¼å¼ï¼Œæ¯è¡Œä¸€ä¸ªæ—¶é—´ç‚¹
# æ–‡ä»¶å: price_data.csv
timestamp,open,high,low,close,volume,adj_close
2024-01-01 09:30:00,150.25,150.80,149.90,150.50,1000000,150.50
2024-01-01 09:31:00,150.50,151.20,150.30,151.00,1200000,151.00
...
```

#### 2. æŠ€æœ¯æŒ‡æ ‡æ•°æ®
```python
# æ–‡ä»¶å: technical_indicators.csv
# 20ä¸ªæŠ€æœ¯æŒ‡æ ‡åˆ—
timestamp,sma_5,sma_10,sma_20,sma_50,ema_12,ema_26,rsi_14,macd_line,macd_signal,macd_hist,bb_upper,bb_lower,bb_percent,vol_sma,vol_ratio,price_vol,momentum_1,momentum_5,momentum_10,hl_spread
2024-01-01 09:30:00,150.1,149.8,148.5,147.2,150.3,149.1,65.5,0.8,0.6,0.2,152.1,148.9,0.6,950000,1.05,2.1,0.002,0.015,0.08,0.006
...
```

#### 3. æ–°é—»æƒ…æ„Ÿæ•°æ®
```python
# æ–‡ä»¶å: news_sentiment.csv
# 768ç»´BERTåµŒå…¥å‘é‡
timestamp,embed_0,embed_1,embed_2,...,embed_767
2024-01-01 09:30:00,0.125,-0.334,0.891,...,0.445
...
```

### æ•°æ®è·å–å»ºè®®

#### å…è´¹æ•°æ®æº
```python
# ä»·æ ¼æ•°æ®
import yfinance as yf
import pandas as pd

# è·å–è‚¡ç¥¨æ•°æ®
ticker = "AAPL"
data = yf.download(ticker, start="2020-01-01", end="2024-01-01", interval="1m")
data.to_csv(f"{ticker}_price_data.csv")
```

#### ä»˜è´¹æ•°æ®æº
- **Alpha Vantage**: å®æ—¶å’Œå†å²æ•°æ®
- **Quandl**: é«˜è´¨é‡é‡‘èæ•°æ®
- **Bloomberg API**: ä¸“ä¸šçº§æ•°æ®
- **Refinitiv Eikon**: æœºæ„çº§æ•°æ®

---

## ğŸ› ï¸ å®‰è£…æ­¥éª¤

### 1. åˆ›å»ºPythonç¯å¢ƒ
```bash
# ä½¿ç”¨conda
conda create -n hnet-stock python=3.10
conda activate hnet-stock

# æˆ–ä½¿ç”¨venv
python -m venv hnet-stock
source hnet-stock/bin/activate  # Linux/Mac
# æˆ– hnet-stock\Scripts\activate  # Windows
```

### 2. å®‰è£…ä¾èµ–åŒ…
```bash
# æ ¸å¿ƒæ·±åº¦å­¦ä¹ æ¡†æ¶
pip install torch==2.1.0 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# æ•°æ®å¤„ç†
pip install pandas==2.1.3
pip install numpy==1.24.3
pip install scikit-learn==1.3.2

# é‡‘èæ•°æ®
pip install yfinance==0.2.24
pip install ta-lib==0.4.28
pip install pandas-ta==0.3.14b

# å¯è§†åŒ–
pip install matplotlib==3.8.2
pip install seaborn==0.13.0
pip install plotly==5.17.0

# æ¨¡å‹ä¼˜åŒ–
pip install optuna==3.4.0
pip install onnx==1.15.0
pip install onnxruntime-gpu==1.16.3

# æ–‡æœ¬å¤„ç†(æ–°é—»æƒ…æ„Ÿ)
pip install transformers==4.36.2
pip install sentence-transformers==2.2.2

# å…¶ä»–å·¥å…·
pip install tqdm==4.66.1
pip install tensorboard==2.15.1
pip install wandb==0.16.1  # å¯é€‰ï¼Œç”¨äºå®éªŒè·Ÿè¸ª
```

### 3. éªŒè¯å®‰è£…
```python
# test_installation.py
import torch
import pandas as pd
import numpy as np

print(f"PyTorch version: {torch.__version__}")
print(f"CUDA available: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"GPU count: {torch.cuda.device_count()}")
    print(f"GPU name: {torch.cuda.get_device_name(0)}")

# æµ‹è¯•åŸºæœ¬åŠŸèƒ½
x = torch.randn(2, 3).cuda() if torch.cuda.is_available() else torch.randn(2, 3)
print("Installation successful!")
```

---

## ğŸ”„ æ•°æ®é¢„å¤„ç†

### 1. åˆ›å»ºæ•°æ®é¢„å¤„ç†è„šæœ¬
```python
# data_preprocessing.py
import pandas as pd
import numpy as np
import ta
from transformers import AutoTokenizer, AutoModel
import torch
import os
from tqdm import tqdm

class StockDataPreprocessor:
    def __init__(self, raw_data_path, output_path):
        self.raw_data_path = raw_data_path
        self.output_path = output_path
        self.sentiment_model = None
        
    def load_price_data(self, symbol):
        """åŠ è½½ä»·æ ¼æ•°æ®"""
        file_path = f"{self.raw_data_path}/{symbol}_price.csv"
        df = pd.read_csv(file_path, parse_dates=['timestamp'])
        return df.sort_values('timestamp')
    
    def compute_technical_indicators(self, df):
        """è®¡ç®—æŠ€æœ¯æŒ‡æ ‡"""
        # ä½¿ç”¨taåº“è®¡ç®—æŠ€æœ¯æŒ‡æ ‡
        indicators = pd.DataFrame()
        
        # ç§»åŠ¨å¹³å‡çº¿
        indicators['sma_5'] = ta.trend.sma_indicator(df['close'], window=5)
        indicators['sma_10'] = ta.trend.sma_indicator(df['close'], window=10)
        indicators['sma_20'] = ta.trend.sma_indicator(df['close'], window=20)
        indicators['sma_50'] = ta.trend.sma_indicator(df['close'], window=50)
        
        # æŒ‡æ•°ç§»åŠ¨å¹³å‡
        indicators['ema_12'] = ta.trend.ema_indicator(df['close'], window=12)
        indicators['ema_26'] = ta.trend.ema_indicator(df['close'], window=26)
        
        # RSI
        indicators['rsi_14'] = ta.momentum.rsi(df['close'], window=14)
        
        # MACD
        macd = ta.trend.MACD(df['close'])
        indicators['macd_line'] = macd.macd()
        indicators['macd_signal'] = macd.macd_signal()
        indicators['macd_hist'] = macd.macd_diff()
        
        # å¸ƒæ—å¸¦
        bb = ta.volatility.BollingerBands(df['close'])
        indicators['bb_upper'] = bb.bollinger_hband()
        indicators['bb_lower'] = bb.bollinger_lband()
        indicators['bb_percent'] = bb.bollinger_pband()
        
        # æˆäº¤é‡æŒ‡æ ‡
        indicators['vol_sma'] = ta.volume.volume_sma(df['close'], df['volume'])
        indicators['vol_ratio'] = df['volume'] / indicators['vol_sma']
        
        # ä»·æ ¼æ³¢åŠ¨ç‡
        indicators['price_vol'] = df['close'].rolling(20).std()
        
        # åŠ¨é‡æŒ‡æ ‡
        indicators['momentum_1'] = df['close'].pct_change(1)
        indicators['momentum_5'] = df['close'].pct_change(5)
        indicators['momentum_10'] = df['close'].pct_change(10)
        
        # é«˜ä½ä»·å·®
        indicators['hl_spread'] = (df['high'] - df['low']) / df['close']
        
        return indicators.fillna(0)
    
    def process_news_sentiment(self, news_file):
        """å¤„ç†æ–°é—»æƒ…æ„Ÿæ•°æ®"""
        if self.sentiment_model is None:
            self.sentiment_model = AutoModel.from_pretrained('ProsusAI/finbert')
            self.tokenizer = AutoTokenizer.from_pretrained('ProsusAI/finbert')
        
        # è¯»å–æ–°é—»æ•°æ®
        news_df = pd.read_csv(news_file)
        
        embeddings = []
        for text in tqdm(news_df['content'], desc="Processing news"):
            inputs = self.tokenizer(text, return_tensors='pt', 
                                  truncation=True, max_length=512)
            with torch.no_grad():
                outputs = self.sentiment_model(**inputs)
                # ä½¿ç”¨[CLS]æ ‡è®°çš„åµŒå…¥
                embedding = outputs.last_hidden_state[:, 0, :].numpy()
                embeddings.append(embedding.flatten())
        
        return np.array(embeddings)
    
    def create_sequences(self, price_data, technical_data, news_data, 
                        sequence_length=1024, prediction_horizon=20):
        """åˆ›å»ºè®­ç»ƒåºåˆ—"""
        sequences = []
        
        total_length = len(price_data)
        
        for i in range(sequence_length, total_length - prediction_horizon):
            # è¾“å…¥åºåˆ—
            price_seq = price_data[i-sequence_length:i]
            tech_seq = technical_data[i-sequence_length:i]
            news_seq = news_data[i-sequence_length:i]
            
            # é¢„æµ‹ç›®æ ‡
            future_prices = price_data[i:i+prediction_horizon, 3]  # close prices
            future_vols = technical_data[i:i+prediction_horizon, -4]  # price volatility
            
            # æ–¹å‘æ ‡ç­¾ (0: ä¸‹è·Œ, 1: æŒå¹³, 2: ä¸Šæ¶¨)
            price_changes = np.diff(future_prices)
            directions = np.where(price_changes < -0.001, 0,
                                np.where(price_changes > 0.001, 2, 1))
            
            sequences.append({
                'price': price_seq,
                'technical': tech_seq,
                'news': news_seq,
                'target_price': future_prices,
                'target_volatility': future_vols,
                'target_direction': directions
            })
        
        return sequences
    
    def save_processed_data(self, sequences, split_ratios=(0.7, 0.15, 0.15)):
        """ä¿å­˜å¤„ç†åçš„æ•°æ®"""
        total_samples = len(sequences)
        train_size = int(total_samples * split_ratios[0])
        val_size = int(total_samples * split_ratios[1])
        
        # åˆ†å‰²æ•°æ®
        train_data = sequences[:train_size]
        val_data = sequences[train_size:train_size + val_size]
        test_data = sequences[train_size + val_size:]
        
        # ä¿å­˜æ•°æ®
        for split, data in [('train', train_data), ('val', val_data), ('test', test_data)]:
            os.makedirs(f"{self.output_path}/{split}", exist_ok=True)
            
            # åˆ†åˆ«ä¿å­˜ä¸åŒç±»å‹çš„æ•°æ®
            price_data = np.array([seq['price'] for seq in data])
            tech_data = np.array([seq['technical'] for seq in data])
            news_data = np.array([seq['news'] for seq in data])
            
            np.save(f"{self.output_path}/{split}/price_data.npy", price_data)
            np.save(f"{self.output_path}/{split}/technical_data.npy", tech_data)
            np.save(f"{self.output_path}/{split}/news_data.npy", news_data)
            
            # ä¿å­˜ç›®æ ‡æ•°æ®
            targets = {
                'price': np.array([seq['target_price'] for seq in data]),
                'volatility': np.array([seq['target_volatility'] for seq in data]),
                'direction': np.array([seq['target_direction'] for seq in data])
            }
            
            np.save(f"{self.output_path}/{split}/targets.npy", targets)
        
        print(f"Data saved: Train: {len(train_data)}, Val: {len(val_data)}, Test: {len(test_data)}")

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    preprocessor = StockDataPreprocessor("raw_data", "processed_data")
    
    # å¤„ç†å•ä¸ªè‚¡ç¥¨
    symbol = "AAPL"
    price_df = preprocessor.load_price_data(symbol)
    technical_df = preprocessor.compute_technical_indicators(price_df)
    
    # å¦‚æœæœ‰æ–°é—»æ•°æ®
    # news_embeddings = preprocessor.process_news_sentiment("raw_data/news.csv")
    # å¦åˆ™ä½¿ç”¨éšæœºåµŒå…¥ä½œä¸ºå ä½ç¬¦
    news_embeddings = np.random.randn(len(price_df), 768)
    
    # åˆ›å»ºåºåˆ—
    sequences = preprocessor.create_sequences(
        price_df[['open', 'high', 'low', 'close', 'volume', 'adj_close']].values,
        technical_df.values,
        news_embeddings
    )
    
    # ä¿å­˜æ•°æ®
    preprocessor.save_processed_data(sequences)
```

### 2. è¿è¡Œæ•°æ®é¢„å¤„ç†
```bash
# åˆ›å»ºç›®å½•ç»“æ„
mkdir -p data/{raw_data,processed_data}

# ä¸‹è½½ç¤ºä¾‹æ•°æ®
python download_sample_data.py  # éœ€è¦åˆ›å»ºæ­¤è„šæœ¬

# è¿è¡Œé¢„å¤„ç†
python data_preprocessing.py
```

---

## ğŸš€ æ¨¡å‹è®­ç»ƒ

### 1. åŸºç¡€è®­ç»ƒ
```bash
# ä½¿ç”¨é»˜è®¤é…ç½®è®­ç»ƒ
python hnet_stock_training.py --mode train --data_path processed_data/

# è‡ªå®šä¹‰é…ç½®è®­ç»ƒ
python hnet_stock_training.py --mode train \
    --data_path processed_data/ \
    --batch_size 16 \
    --learning_rate 5e-5 \
    --max_epochs 50
```

### 2. åˆ†å¸ƒå¼è®­ç»ƒï¼ˆå¤šGPUï¼‰
```bash
# ä½¿ç”¨torch.distributed
torchrun --nproc_per_node=4 hnet_stock_training.py \
    --mode train \
    --data_path processed_data/ \
    --batch_size 8  # æ¯ä¸ªGPUçš„batch size
```

### 3. ä½¿ç”¨é…ç½®æ–‡ä»¶è®­ç»ƒ
```yaml
# config.yaml
model:
  d_model: 512
  num_stages: 2
  encoder_layers: 4
  decoder_layers: 4
  main_layers: 16
  chunk_ratios: [4, 3]
  dropout: 0.1

training:
  batch_size: 16
  learning_rate: 5e-5
  weight_decay: 1e-5
  max_epochs: 100
  warmup_steps: 1000

data:
  sequence_length: 1024
  prediction_horizon: 20
  price_features: 6
  technical_features: 20
  news_embed_dim: 768
```

```python
# train_with_config.py
import yaml
from hnet_stock_training import HNetConfig, StockTrainer, StockDataset

def load_config(config_path):
    with open(config_path, 'r') as f:
        config_dict = yaml.safe_load(f)
    
    # åˆå¹¶é…ç½®
    config = HNetConfig()
    for section, params in config_dict.items():
        for key, value in params.items():
            if hasattr(config, key):
                setattr(config, key, value)
    
    return config

if __name__ == "__main__":
    config = load_config("config.yaml")
    trainer = StockTrainer(config)
    
    train_dataset = StockDataset('processed_data/train/', config)
    val_dataset = StockDataset('processed_data/val/', config)
    
    trainer.train(train_dataset, val_dataset)
```

### 4. è¶…å‚æ•°ä¼˜åŒ–
```bash
# è‡ªåŠ¨è¶…å‚æ•°æœç´¢
python hnet_stock_training.py --mode optimize --data_path processed_data/
```

### 5. è®­ç»ƒç›‘æ§
```python
# ä½¿ç”¨TensorBoard
tensorboard --logdir runs/

# ä½¿ç”¨Weights & Biases (å¯é€‰)
import wandb

wandb.init(project="hnet-stock-analysis")
# åœ¨è®­ç»ƒä»£ç ä¸­æ·»åŠ æ—¥å¿—è®°å½•
wandb.log({"train_loss": train_loss, "val_loss": val_loss})
```

---

## ğŸ”® å®æ—¶æ¨ç†

### 1. æ¨¡å‹åŠ è½½å’Œæ¨ç†
```python
# real_time_inference.py
from hnet_stock_training import RealTimeInference, HNetConfig
import numpy as np
import time

# åˆå§‹åŒ–æ¨ç†å¼•æ“
config = HNetConfig()
inference = RealTimeInference("best_stock_hnet.pth", config)

# æ¨¡æ‹Ÿå®æ—¶æ•°æ®æµ
while True:
    # è·å–æ–°æ•°æ® (è¿™é‡Œéœ€è¦è¿æ¥åˆ°å®é™…æ•°æ®æº)
    new_price = np.random.randn(6)  # OHLCVA
    new_technical = np.random.randn(20)  # æŠ€æœ¯æŒ‡æ ‡
    new_news = np.random.randn(768)  # æ–°é—»åµŒå…¥
    
    # æ›´æ–°æ•°æ®ç¼“å†²åŒº
    inference.update_data(new_price, new_technical, new_news)
    
    # è¿›è¡Œé¢„æµ‹
    predictions = inference.predict()
    
    print(f"Price forecast: {predictions['price_forecast'][:5]}")
    print(f"Volatility forecast: {predictions['volatility_forecast'][:5]}")
    print(f"Direction probabilities: {predictions['direction_probs'][:5]}")
    
    time.sleep(60)  # æ¯åˆ†é’Ÿé¢„æµ‹ä¸€æ¬¡
```

### 2. REST APIæœåŠ¡
```python
# api_server.py
from flask import Flask, request, jsonify
from hnet_stock_training import RealTimeInference, HNetConfig
import numpy as np

app = Flask(__name__)

# åˆå§‹åŒ–æ¨¡å‹
config = HNetConfig()
inference = RealTimeInference("best_stock_hnet.pth", config)

@app.route('/predict', methods=['POST'])
def predict():
    data = request.json
    
    # è§£æè¾“å…¥æ•°æ®
    price_data = np.array(data['price'])
    technical_data = np.array(data['technical'])
    news_data = np.array(data['news'])
    
    # æ›´æ–°æ•°æ®å¹¶é¢„æµ‹
    inference.update_data(price_data, technical_data, news_data)
    predictions = inference.predict()
    
    return jsonify(predictions)

@app.route('/health', methods=['GET'])
def health():
    return jsonify({"status": "healthy"})

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000)
```

### 3. ä½¿ç”¨Dockeréƒ¨ç½²
```dockerfile
# Dockerfile
FROM nvidia/cuda:11.8-runtime-ubuntu20.04

WORKDIR /app

COPY requirements.txt .
RUN pip install -r requirements.txt

COPY . .

EXPOSE 5000

CMD ["python", "api_server.py"]
```

```bash
# æ„å»ºå’Œè¿è¡Œ
docker build -t hnet-stock-api .
docker run -p 5000:5000 --gpus all hnet-stock-api
```

---

## âš¡ æ€§èƒ½ä¼˜åŒ–

### 1. æ¨¡å‹é‡åŒ–
```python
# quantize_model.py
from hnet_stock_training import ModelOptimizer, StockDataset, HNetConfig

config = HNetConfig()
dataset = StockDataset('processed_data/test/', config)
dataloader = DataLoader(dataset, batch_size=1)

# é‡åŒ–æ¨¡å‹
quantized_model = ModelOptimizer.quantize_model(model, dataloader)

# ä¿å­˜é‡åŒ–æ¨¡å‹
torch.save(quantized_model.state_dict(), 'quantized_model.pth')
```

### 2. ONNXå¯¼å‡º
```python
# export_onnx.py
from hnet_stock_training import ModelOptimizer, StockHNet, HNetConfig

config = HNetConfig()
model = StockHNet(config)
model.load_state_dict(torch.load('best_stock_hnet.pth')['model_state_dict'])

# å¯¼å‡ºONNX
ModelOptimizer.export_to_onnx(model, config, 'stock_hnet.onnx')
```

### 3. æ¨ç†åŠ é€Ÿ
```python
# ä½¿ç”¨ONNX Runtimeæ¨ç†
import onnxruntime as ort

session = ort.InferenceSession('stock_hnet.onnx', providers=['CUDAExecutionProvider'])

def fast_predict(price_data, technical_data, news_data):
    inputs = {
        'price_data': price_data.numpy(),
        'technical_data': technical_data.numpy(),
        'news_data': news_data.numpy()
    }
    
    outputs = session.run(None, inputs)
    return outputs[0]
```

---

## ğŸ”§ æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ

#### 1. å†…å­˜ä¸è¶³
```bash
# å‡å°batch size
python hnet_stock_training.py --batch_size 8

# ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯
python hnet_stock_training.py --gradient_accumulation_steps 4
```

#### 2. CUDAå†…å­˜é”™è¯¯
```python
# åœ¨ä»£ç ä¸­æ·»åŠ å†…å­˜æ¸…ç†
torch.cuda.empty_cache()

# ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()
with autocast():
    outputs = model(inputs)
    loss = criterion(outputs, targets)

scaler.scale(loss).backward()
scaler.step(optimizer)
scaler.update()
```

#### 3. è®­ç»ƒä¸æ”¶æ•›
- æ£€æŸ¥å­¦ä¹ ç‡è®¾ç½®
- éªŒè¯æ•°æ®é¢„å¤„ç†æ˜¯å¦æ­£ç¡®
- å°è¯•ä¸åŒçš„ä¼˜åŒ–å™¨

#### 4. æ•°æ®åŠ è½½æ…¢
```python
# å¢åŠ æ•°æ®åŠ è½½workeræ•°é‡
dataloader = DataLoader(dataset, batch_size=32, num_workers=8, pin_memory=True)
```

### æ—¥å¿—å’Œè°ƒè¯•
```python
import logging

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('training.log'),
        logging.StreamHandler()
    ]
)
```

---

## ğŸ“š æ€§èƒ½åŸºå‡†æµ‹è¯•

### é¢„æœŸæ€§èƒ½æŒ‡æ ‡
```
åœ¨æ ‡å‡†é…ç½®ä¸‹ï¼ˆRTX 4090ï¼Œ16GBæ•°æ®ï¼‰:
- è®­ç»ƒæ—¶é—´: ~2-3å°æ—¶/epoch
- æ¨ç†å»¶è¿Ÿ: <50ms
- å†…å­˜ä½¿ç”¨: ~18GB GPUå†…å­˜
- æ–¹å‘å‡†ç¡®ç‡: >55%
- ä»·æ ¼é¢„æµ‹MAE: <0.5%
```

### åŸºå‡†æµ‹è¯•è„šæœ¬
```python
# benchmark.py
import time
import torch
from hnet_stock_training import StockHNet, HNetConfig

def benchmark_inference(model, config, num_samples=1000):
    model.eval()
    
    # å‡†å¤‡æµ‹è¯•æ•°æ®
    price_data = torch.randn(1, config.sequence_length, config.price_features)
    technical_data = torch.randn(1, config.sequence_length, config.technical_features)
    news_data = torch.randn(1, config.sequence_length, config.news_embed_dim)
    
    # é¢„çƒ­
    for _ in range(10):
        with torch.no_grad():
            _ = model(price_data, technical_data, news_data)
    
    # åŸºå‡†æµ‹è¯•
    start_time = time.time()
    for _ in range(num_samples):
        with torch.no_grad():
            _ = model(price_data, technical_data, news_data)
    
    end_time = time.time()
    avg_latency = (end_time - start_time) / num_samples * 1000  # ms
    
    print(f"Average inference latency: {avg_latency:.2f}ms")
    return avg_latency

if __name__ == "__main__":
    config = HNetConfig()
    model = StockHNet(config)
    benchmark_inference(model, config)
```

---

## ğŸ¯ ä¸‹ä¸€æ­¥å»ºè®®

1. **æ•°æ®è´¨é‡æå‡**: é›†æˆæ›´å¤šé«˜è´¨é‡æ•°æ®æº
2. **æ¨¡å‹æ¶æ„ä¼˜åŒ–**: å°è¯•ä¸åŒçš„å‹ç¼©æ¯”ä¾‹å’Œå±‚æ•°é…ç½®
3. **ç‰¹å¾å·¥ç¨‹**: æ·»åŠ æ›´å¤šé‡‘èç‰¹å®šç‰¹å¾
4. **åœ¨çº¿å­¦ä¹ **: å®ç°å¢é‡å­¦ä¹ æœºåˆ¶
5. **é£é™©ç®¡ç†**: æ·»åŠ ä¸ç¡®å®šæ€§é‡åŒ–

---

**æ³¨æ„**: è¿™æ˜¯ä¸€ä¸ªç ”ç©¶åŸå‹ï¼Œå®é™…éƒ¨ç½²å‰è¯·è¿›è¡Œå……åˆ†çš„å›æµ‹å’Œé£é™©è¯„ä¼°ã€‚é‡‘èå¸‚åœºé¢„æµ‹å­˜åœ¨å›ºæœ‰é£é™©ï¼Œä½¿ç”¨æ—¶è¯·è°¨æ…ã€‚